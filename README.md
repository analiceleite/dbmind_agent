# ğŸ¤– Zypher Agent Chat Application

A full-stack AI chat application using Deno, React, Zypher Agent, and Ollama (Llama 3.2) running locally.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  React Frontend (Port 5174)                     â”‚
â”‚  - Chat interface                               â”‚
â”‚  - WebSocket client                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ WebSocket
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Deno Backend (Port 8000)                       â”‚
â”‚  - WebSocket server                             â”‚
â”‚  - Zypher Agent orchestration                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ HTTP/API
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ollama Container (Port 11434)                  â”‚
â”‚  - Llama 3.2 model                              â”‚
â”‚  - Local AI inference                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### 1. Start Ollama (Docker)

```bash
# Start the Ollama container
docker-compose up -d

# Pull the Llama 3.2 model
docker exec -it ollama ollama pull phi3:mini

# Verify it's running
docker exec -it ollama ollama list
```

### 2. Start Backend (Deno)

```bash
cd backend
deno task start
```

The backend will be available at `http://localhost:8000`

### 3. Start Frontend (React)

```bash
cd frontend
npm install
npm run dev
```

The frontend will be available at `http://localhost:5174`

## ğŸ“Š Data Flow

### User asks a question:

1. **User types** â†’ Message sent via WebSocket to backend
2. **Backend receives** â†’ Deno WebSocket server processes request
3. **Zypher Agent** â†’ Orchestrates the task and calls Ollama
4. **Ollama processes** â†’ Llama 3.2 generates AI response
5. **Response returns** â†’ Complete answer sent back through WebSocket
6. **Frontend displays** â†’ User sees the AI's response

### Technical Flow:

```typescript
// 1. Frontend sends message
WebSocket â†’ { type: "task", task: "Hello!", model: "phi3:mini" }

// 2. Backend shows loading
WebSocket â† { type: "status", message: "Processing task..." }

// 3. Zypher Agent calls Ollama
agent.runTask("Hello!", "phi3:mini")
  â†’ Ollama API at localhost:11434
  â†’ Llama 3.2 generates response

// 4. Backend sends complete response
WebSocket â† { type: "complete", message: "Hi! How can I help?" }

// 5. Frontend displays message
```

## ğŸ› ï¸ Technology Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Runtime** | Deno | TypeScript/JavaScript runtime for backend |
| **Backend** | Zypher Agent | AI agent orchestration framework |
| **LLM** | Ollama (Llama 3.2) | Local AI model for generating responses |
| **Frontend** | React + Vite | User interface |
| **Communication** | WebSocket | Real-time bidirectional communication |
| **Containerization** | Docker | Running Ollama in isolated environment |

## ğŸ“ Project Structure

```
zypher_agent/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ server.ts           # Deno WebSocket server
â”‚   â”œâ”€â”€ deno.json           # Deno configuration
â”‚   â””â”€â”€ .env                # Environment variables
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx         # Main React component
â”‚   â”‚   â”œâ”€â”€ App.css         # Styles
â”‚   â”‚   â””â”€â”€ hooks/
â”‚   â”‚       â””â”€â”€ useZypherAgent.ts  # WebSocket hook
â”‚   â””â”€â”€ package.json        # Frontend dependencies
â”‚
â”œâ”€â”€ docker-compose.yml      # Ollama container config
â””â”€â”€ README.md              # This file
```

## ğŸ”‘ Key Components

### Zypher Agent
- AI agent framework that orchestrates complex tasks
- Connects to different LLM providers (Ollama, OpenAI, Claude)
- Manages context, memory, and tool usage

### Ollama
- Runs open-source LLMs locally
- Compatible with OpenAI API format
- Free and private (no data leaves your machine)

### Deno
- Modern TypeScript/JavaScript runtime
- Secure by default (explicit permissions)
- Native TypeScript support

## ğŸ¯ Features

- âœ… Real-time chat with AI
- âœ… Local AI processing (private and free)
- âœ… Simple and clean UI
- âœ… WebSocket for instant responses
- âœ… Loading indicator while processing
- âœ… Complete responses (no streaming chunks)

## ğŸ› Troubleshooting

**Ollama not responding:**
```bash
docker-compose logs ollama
docker-compose restart ollama
```

**Backend errors:**
```bash
# Check if Ollama is running
curl http://localhost:11434/api/version

# Restart backend
cd backend
deno task start
```

**Frontend not connecting:**
- Check if backend is running on port 8000
- Check browser console (F12) for WebSocket errors

## ğŸ“š Learn More

- [Deno Documentation](https://deno.land/)
- [Zypher Agent](https://jsr.io/@corespeed/zypher)
- [Ollama](https://ollama.ai/)
- [Llama 3.2](https://llama.meta.com/)

## ğŸ“ License

MIT
