# ğŸ¤– Zypher Agent Chat Application

A full-stack AI chat application using Deno, React, and Zypher Agent with local AI models.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  React Frontend (Port 5174)                     â”‚
â”‚  - Chat interface with real-time streaming      â”‚
â”‚  - WebSocket client                             â”‚
â”‚  - Markdown rendering & syntax highlighting     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ WebSocket
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Deno Backend (Port 8000)                       â”‚
â”‚  - WebSocket server                             â”‚
â”‚  - Zypher Agent with intelligent task routing   â”‚
â”‚  - Modular provider architecture                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### 1. Setup Ollama (LLM Provider)

O Zypher Agent usa Ollama como provider de modelos locais:

```bash
# Instalar Ollama (Windows/Mac/Linux)
# Visite: https://ollama.ai/download

# Ou usar Docker
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama

# Baixar um modelo (recomendado: Phi-3 Mini)
ollama pull phi3:mini

# Verificar modelos disponÃ­veis
ollama list

# Testar se estÃ¡ funcionando
curl http://localhost:11434/api/version
```

### 2. Start Backend (Deno)

```bash
cd backend
deno task start
```

The backend will be available at `http://localhost:8000`

### 3. Start Frontend (React)

```bash
cd frontend
npm install
npm run dev
```

The frontend will be available at `http://localhost:5174`

## ğŸ“Š Data Flow

### User asks a question:

1. **User types** â†’ Message sent via WebSocket to backend
2. **Backend receives** â†’ Deno WebSocket server processes request
3. **Zypher Agent** â†’ Intelligently processes the task with AI capabilities
4. **AI processes** â†’ Local model generates contextual response with streaming
5. **Response streams** â†’ Real-time chunks sent back through WebSocket
6. **Frontend renders** â†’ Markdown formatted response with syntax highlighting

### Technical Flow:

```typescript
// 1. Frontend sends message
WebSocket â†’ { type: "task", task: "Hello!", model: "phi3:mini" }

// 2. Backend shows loading
WebSocket â† { type: "status", message: "Processing task..." }

// 3. Zypher Agent processes with streaming
agent.runTaskStream("Hello!", "phi3:mini")
  â†’ Intelligent task routing and context management
  â†’ Streaming response generation

// 4. Backend sends streaming chunks
WebSocket â† { type: "complete", message: "Hi! How can I help?" }

// 5. Frontend displays message
```

## ğŸ› ï¸ Technology Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Runtime** | Deno | TypeScript/JavaScript runtime for backend |
| **AI Framework** | Zypher Agent | Intelligent AI agent orchestration and task routing |
| **AI Models** | Local Models | Phi-3 Mini and other compatible models |
| **Frontend** | React + Vite | Modern UI with real-time streaming |
| **Communication** | WebSocket | Real-time bidirectional streaming communication |
| **Styling** | Styled Components | Modular component-based styling |

## ğŸ“ Project Structure

```
zypher_agent/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ server.ts           # Deno WebSocket server
â”‚   â”œâ”€â”€ deno.json           # Deno configuration
â”‚   â””â”€â”€ .env                # Environment variables
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx         # Main React component
â”‚   â”‚   â”œâ”€â”€ App.css         # Styles
â”‚   â”‚   â””â”€â”€ hooks/
â”‚   â”‚       â””â”€â”€ useZypherAgent.ts  # WebSocket hook
â”‚   â””â”€â”€ package.json        # Frontend dependencies
â”‚
â”œâ”€â”€ infrastructure/         # Optional database config
â””â”€â”€ README.md              # This file
```

## ğŸ”‘ Key Components

### Zypher Agent
- Advanced AI agent framework for intelligent task orchestration
- Supports multiple LLM providers with OpenAI-compatible APIs
- Manages context, memory, and tool usage with streaming capabilities
- Provides intelligent routing and response optimization

### Deno
- Modern TypeScript/JavaScript runtime
- Secure by default (explicit permissions)
- Native TypeScript support

## ğŸ¯ Features

- âœ… Real-time streaming chat with intelligent AI agents
- âœ… Advanced markdown rendering with syntax highlighting
- âœ… Dark/light theme support with smooth transitions
- âœ… WebSocket streaming for fluid response delivery
- âœ… Modular provider architecture with Zypher focus
- âœ… Enhanced UI with particles effects and animations

## âš™ï¸ ConfiguraÃ§Ã£o do Ollama

O Zypher Agent usa Ollama como LLM provider atravÃ©s de uma API compatÃ­vel com OpenAI:

### ConfiguraÃ§Ã£o atual no server.ts:
```typescript
const agent = new ZypherAgent(
  zypherContext,
  new OpenAIModelProvider({
    apiKey: 'not-needed', // Ollama nÃ£o precisa de API key
    baseUrl: 'http://localhost:11434/v1', // Endpoint compatÃ­vel com OpenAI
    openaiClientOptions: {
      maxRetries: 2,
      timeout: 60000,
    }
  }),
);
```

### Modelos recomendados:
- **phi3:mini** - RÃ¡pido e eficiente (3.8B parÃ¢metros)
- **llama3.2:3b** - Boa qualidade geral
- **codellama:7b** - Especializado em cÃ³digo
- **mistral:7b** - Excelente para conversaÃ§Ã£o

### VariÃ¡veis de ambiente (opcional):
```bash
# .env
ZYPHER_BASE_URL=http://localhost:11434
DEFAULT_MODEL=phi3:mini
PORT=8000
```

## ğŸ› Troubleshooting

**Ollama nÃ£o estÃ¡ respondendo:**
```bash
# Verificar se Ollama estÃ¡ rodando
curl http://localhost:11434/api/version

# Iniciar Ollama
ollama serve

# Ou via Docker
docker start ollama

# Verificar logs
docker logs ollama
```

**Zypher Agent not responding:**
```bash
# Check backend logs
cd backend
deno task start

# Verify model is available
ollama list

# Test Ollama directly
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi3:mini",
    "messages": [{"role": "user", "content": "Hello"}],
    "stream": false
  }'
```

**Frontend issues:**
```bash
# Restart frontend development server
cd frontend
npm run dev

# Clear browser cache and refresh
```

**Frontend not connecting:**
- Check if backend is running on port 8000
- Check browser console (F12) for WebSocket errors

## ğŸ“š Learn More

- [Deno Documentation](https://deno.land/)
- [Zypher Agent](https://jsr.io/@corespeed/zypher)
- [Ollama](https://ollama.ai/)
- [Llama 3.2](https://llama.meta.com/)

## ğŸ“ License

MIT
